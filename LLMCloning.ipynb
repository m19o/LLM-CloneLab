{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()  # This will prompt you for your HuggingFace token\n",
    "logging.basicConfig(filename='active_learning_adversary.log', level=logging.INFO, format='%(asctime)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba9bf0e9",
   "metadata": {},
   "source": [
    "# Cell 2: Generate a large, diverse prompt pool\n",
    "\n",
    "categories = {\n",
    "    \"geography\": [\"What is the capital of {}?\", \"Where is {} located?\", \"Name a famous landmark in {}.\"],\n",
    "    \"science\": [\"Explain the concept of {}.\", \"What is {} used for?\", \"Describe the process of {}.\"],\n",
    "    \"history\": [\"Who was {}?\", \"What happened in {}?\", \"Describe the significance of {}.\"],\n",
    "    \"general\": [\"Tell me a joke about {}.\", \"Write a poem about {}.\", \"What are the benefits of {}?\", \"How do you make {}?\", \"Write a story about {}.\"]\n",
    "}\n",
    "fillins = [\n",
    "    \"France\", \"gravity\", \"Einstein\", \"the moon\", \"electricity\", \"World War II\", \"Python programming\", \"the internet\", \"Mount Everest\", \"photosynthesis\",\n",
    "    \"the Great Wall of China\", \"Shakespeare\", \"the human brain\", \"Africa\", \"pancakes\", \"democracy\", \"pizza\", \"the sun\", \"the ocean\", \"the heart\"\n",
    "]\n",
    "candidate_prompts = []\n",
    "for cat in categories:\n",
    "    for template in categories[cat]:\n",
    "        for fill in fillins:\n",
    "            candidate_prompts.append(template.format(fill))\n",
    "random.shuffle(candidate_prompts)\n",
    "print(f\"Generated {len(candidate_prompts)} prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2964ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Query the victim model (GPT-2) and build the dataset\n",
    "victim_model_name = \"gpt2\"\n",
    "victim_model = AutoModelForCausalLM.from_pretrained(victim_model_name)\n",
    "victim_tokenizer = AutoTokenizer.from_pretrained(victim_model_name)\n",
    "\n",
    "victim_outputs = {}\n",
    "max_length = 50\n",
    "for prompt in candidate_prompts[:50]: \n",
    "    input_ids = victim_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    with torch.no_grad():\n",
    "        output_ids = victim_model.generate(input_ids, max_length=max_length, pad_token_id=victim_tokenizer.eos_token_id)\n",
    "    output = victim_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if output.startswith(prompt):\n",
    "        output = output[len(prompt):].strip()\n",
    "    victim_outputs[prompt] = output\n",
    "\n",
    "# Save dataset for later use\n",
    "with open(\"victim_adversary_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(list(victim_outputs.items()), f)\n",
    "print(f\"Collected {len(victim_outputs)} (prompt, output) pairs from the victim.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare the adversary training file\n",
    "with open(\"adversary_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for prompt, output in victim_outputs.items():\n",
    "        f.write(f\"{prompt} ### {output} <|endoftext|>\\n\")\n",
    "print(\"adversary training file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Custom PyTorch Dataset for adversary training\n",
    "class PromptOutputDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size=64):\n",
    "        self.examples = []\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                tokenized = tokenizer(line, truncation=True, max_length=block_size, return_tensors=\"pt\")\n",
    "                self.examples.append(tokenized.input_ids[0])\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train the adversary model (GPT-Neo 125M)\n",
    "adversary_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "adversary_model = AutoModelForCausalLM.from_pretrained(adversary_model_name)\n",
    "adversary_tokenizer = AutoTokenizer.from_pretrained(adversary_model_name)\n",
    "if adversary_tokenizer.pad_token is None:\n",
    "    adversary_tokenizer.pad_token = adversary_tokenizer.eos_token\n",
    "\n",
    "train_dataset = PromptOutputDataset(\"adversary_train.txt\", adversary_tokenizer, block_size=64)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=adversary_tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./adversary_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    report_to=[],\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=adversary_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./adversary_model\")\n",
    "print(\"adversary model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Evaluate the adversary model\n",
    "def evaluate_adversary(prompts, victim_outputs, adversary_model, adversary_tokenizer, max_length=50):\n",
    "    for prompt in prompts:\n",
    "        input_text = f\"{prompt} ###\"\n",
    "        input_ids = adversary_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        attention_mask = (input_ids != adversary_tokenizer.pad_token_id).long()\n",
    "        with torch.no_grad():\n",
    "            output_ids = adversary_model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=adversary_tokenizer.pad_token_id,\n",
    "                eos_token_id=adversary_tokenizer.eos_token_id\n",
    "            )\n",
    "        output = adversary_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        if \"###\" in output:\n",
    "            output = output.split(\"###\", 1)[-1].strip()\n",
    "        if \"<|endoftext|>\" in output:\n",
    "            output = output.split(\"<|endoftext|>\", 1)[0].strip()\n",
    "        print(f\"Prompt: {prompt}\\nVictim Output: {victim_outputs[prompt]}\\nadversary Output: {output}\\n{'='*40}\")\n",
    "\n",
    "# Load the trained adversary model\n",
    "adversary_model = AutoModelForCausalLM.from_pretrained(\"./adversary_model\")\n",
    "adversary_tokenizer = AutoTokenizer.from_pretrained(adversary_model_name)\n",
    "if adversary_tokenizer.pad_token is None:\n",
    "    adversary_tokenizer.pad_token = adversary_tokenizer.eos_token\n",
    "\n",
    "# Evaluate on a sample of prompts\n",
    "sample_prompts = list(victim_outputs.keys())[:10]\n",
    "evaluate_adversary(sample_prompts, victim_outputs, adversary_model, adversary_tokenizer, max_length=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
